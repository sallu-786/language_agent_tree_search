{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **LATS Implementation without External Web Search Tools**"
      ],
      "metadata": {
        "id": "WNSy5ZpSx63L"
      },
      "id": "WNSy5ZpSx63L"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Setting Up the Environment"
      ],
      "metadata": {
        "id": "3tB8RvsvxVgi"
      },
      "id": "3tB8RvsvxVgi"
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations\n",
        "import getpass\n",
        "import os\n",
        "import json\n",
        "import math\n",
        "from collections import deque\n",
        "from typing import Optional, Literal\n",
        "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage\n",
        "from typing_extensions import TypedDict\n",
        "from langgraph.graph import END, StateGraph, START\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.output_parsers.openai_tools import (\n",
        "    JsonOutputToolsParser,\n",
        "    PydanticToolsParser,\n",
        ")\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from langchain_core.runnables import chain as as_runnable\n",
        "from langchain_core.prompt_values import ChatPromptValue\n",
        "from langchain_core.runnables import RunnableConfig\n",
        "from IPython.display import Image, display, Markdown\n",
        "from collections import defaultdict\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4\", openai_api_key=\"\") #enter open-Ai key here"
      ],
      "metadata": {
        "id": "_ree_9koxSHc"
      },
      "id": "_ree_9koxSHc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Class Declarations of Node, Tree State and Reflection"
      ],
      "metadata": {
        "id": "efayTQQDyzwE"
      },
      "id": "efayTQQDyzwE"
    },
    {
      "cell_type": "code",
      "source": [
        "class Node:\n",
        "    def __init__(\n",
        "        self,\n",
        "        messages: list[BaseMessage],\n",
        "        reflection: Reflection,\n",
        "        parent: Optional[Node] = None,\n",
        "    ):\n",
        "        self.messages = messages\n",
        "        self.parent = parent\n",
        "        self.children = []\n",
        "        self.value = 0\n",
        "        self.visits = 0\n",
        "        self.reflection = reflection\n",
        "        self.depth = parent.depth + 1 if parent is not None else 1\n",
        "        self._is_solved = reflection.found_solution if reflection else False\n",
        "        if self._is_solved:\n",
        "            self._mark_tree_as_solved()\n",
        "        self.backpropagate(reflection.normalized_score)\n",
        "        print(f\"Created node : {self}\")\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return (\n",
        "            f\"<Node value={self.value:.2f}, visits={self.visits},\"\n",
        "            f\" Response={self.messages[-1].content[:50] if self.messages else 'No messages'}...,\"\n",
        "            f\" Reflection={self.reflection.reflections[:50] if self.reflection else 'No reflection'}...,\"\n",
        "            f\" is_solved={self._is_solved}, depth={self.depth}>\"\n",
        "        )\n",
        "\n",
        "    @property\n",
        "    def is_solved(self):\n",
        "        return self._is_solved\n",
        "\n",
        "    @property\n",
        "    def is_terminal(self):\n",
        "        return not self.children\n",
        "\n",
        "    @property\n",
        "    def best_child(self):\n",
        "        if not self.children:\n",
        "            return None\n",
        "        all_nodes = self._get_all_children()\n",
        "        return max(all_nodes, key=lambda child: child.upper_confidence_bound())\n",
        "\n",
        "    @property\n",
        "    def best_child_score(self):\n",
        "        if not self.children:\n",
        "            return None\n",
        "        return max(self.children, key=lambda child: int(child.is_solved) * child.value)\n",
        "\n",
        "    @property\n",
        "    def height(self) -> int:\n",
        "        if self.children:\n",
        "            return 1 + max([child.height for child in self.children])\n",
        "        return 1\n",
        "\n",
        "    def upper_confidence_bound(self, exploration_weight=1.0):\n",
        "        if self.parent is None:\n",
        "            raise ValueError(\"Cannot obtain UCT from root node\")\n",
        "        if self.visits == 0:\n",
        "            return float('inf')\n",
        "        average_reward = self.value / self.visits\n",
        "        exploration_term = math.sqrt(math.log(self.parent.visits) / self.visits)\n",
        "        return average_reward + exploration_weight * exploration_term\n",
        "\n",
        "    def backpropagate(self, reward: float):\n",
        "        node = self\n",
        "        while node:\n",
        "            node.visits += 1\n",
        "            node.value = (node.value * (node.visits - 1) + reward) / node.visits\n",
        "            node = node.parent\n",
        "\n",
        "    def get_messages(self, include_reflections: bool = True):\n",
        "        if include_reflections:\n",
        "            return self.messages + [self.reflection.as_message()]\n",
        "        return self.messages\n",
        "\n",
        "    def get_trajectory(self, include_reflections: bool = True) -> list[BaseMessage]:\n",
        "        messages = []\n",
        "        node = self\n",
        "        while node:\n",
        "            messages.extend(\n",
        "                node.get_messages(include_reflections=include_reflections)[::-1]\n",
        "            )\n",
        "            node = node.parent\n",
        "        return messages[::-1]\n",
        "\n",
        "    def _get_all_children(self):\n",
        "        all_nodes = []\n",
        "        nodes = deque([self])\n",
        "        while nodes:\n",
        "            node = nodes.popleft()\n",
        "            all_nodes.extend(node.children)\n",
        "            nodes.extend(node.children)\n",
        "        return all_nodes\n",
        "\n",
        "    def get_best_solution(self):\n",
        "        all_nodes = [self] + self._get_all_children()\n",
        "        best_node = max(\n",
        "            all_nodes,\n",
        "            key=lambda node: int(node.is_terminal and node.is_solved) * node.value,\n",
        "        )\n",
        "        return best_node\n",
        "\n",
        "    def _mark_tree_as_solved(self):\n",
        "        parent = self.parent\n",
        "        while parent:\n",
        "            parent._is_solved = True\n",
        "            parent = parent.parent\n",
        "#---------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "class Reflection(BaseModel):\n",
        "    reflections: str = Field(\n",
        "        description=\"The critique and reflections on the sufficiency, superfluency,\"\n",
        "        \" and general quality of the response.\"\n",
        "    )\n",
        "    score: int = Field(\n",
        "        description=\"Score from 0-10 on the quality of the candidate response.\",\n",
        "        ge=0,\n",
        "        le=10,\n",
        "    )\n",
        "\n",
        "    found_solution: bool = Field(\n",
        "        description=\"Whether the response has fully and perfectly solved the question or task.\\\n",
        "         This should never be true unless an except exceptional answer is generated\")\n",
        "\n",
        "    def as_message(self):\n",
        "        return HumanMessage(\n",
        "            content=f\"Reasoning: {self.reflections}\\nScore: {self.score}\"\n",
        "        )\n",
        "\n",
        "    @property\n",
        "    def normalized_score(self) -> float:\n",
        "        return self.score / 10.0\n",
        "\n",
        "\n",
        "#---------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "class TreeState(TypedDict):\n",
        "    root: Node\n",
        "    input: str\n"
      ],
      "metadata": {
        "id": "CuDsAhB1y3uR"
      },
      "id": "CuDsAhB1y3uR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Reflection"
      ],
      "metadata": {
        "id": "W5uJOV962jje"
      },
      "id": "W5uJOV962jje"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"Reflect and grade the assistant response to the user question below. \\\n",
        "             Be highly critical in response and dont be satisfied easily\\\n",
        "             Check for following critera 1. Relevance to the question 2. Factual Correctness 3. Quality of text\",\n",
        "        ),\n",
        "        (\"user\", \"{input}\"),\n",
        "        MessagesPlaceholder(variable_name=\"candidate\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "reflection_llm_chain = (\n",
        "    prompt\n",
        "    | llm.bind_tools(tools=[Reflection], tool_choice=\"Reflection\").with_config(\n",
        "        run_name=\"Reflection\"\n",
        "    )\n",
        "    | PydanticToolsParser(tools=[Reflection])\n",
        ")\n",
        "@as_runnable\n",
        "def reflection_chain(inputs) -> Reflection:\n",
        "    tool_choices = reflection_llm_chain.invoke(inputs)\n",
        "    reflection = tool_choices[0]\n",
        "    if not isinstance(inputs[\"candidate\"][-1], AIMessage):\n",
        "        reflection.found_solution = False\n",
        "    print(f\"Generated reflection: {reflection} \\n\")\n",
        "    return reflection\n"
      ],
      "metadata": {
        "id": "hE-JCSBkzPee"
      },
      "id": "hE-JCSBkzPee",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Initial Response with Reflection"
      ],
      "metadata": {
        "id": "4xPlGuiL4tJM"
      },
      "id": "4xPlGuiL4tJM"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are an AI assistant. Your job is to answer user question in an accurate and concise manner \",\n",
        "        ),\n",
        "        (\"user\", \"{input}\"),\n",
        "        MessagesPlaceholder(variable_name=\"messages\", optional=True),\n",
        "    ]\n",
        ")\n",
        "\n",
        "initial_answer_chain = prompt_template | llm.with_config(run_name=\"GenerateInitialCandidate\")\n",
        "\n",
        "parser = JsonOutputToolsParser(return_id=True)\n",
        "\n",
        "def generate_initial_response(state: TreeState) -> dict:\n",
        "    print(\"Generating initial response\")\n",
        "    res = initial_answer_chain.invoke({\"input\": state[\"input\"]})\n",
        "    output_messages = [res]\n",
        "    content = res.content\n",
        "    display(Markdown(content))\n",
        "    # print(f\"Initial response: {res.content[:100]}...\")\n",
        "    reflection = reflection_chain.invoke(\n",
        "        {\"input\": state[\"input\"], \"candidate\": output_messages}\n",
        "    )\n",
        "    # print(f\"\\nInitial reflection: {reflection} \\n \")\n",
        "    root = Node(output_messages, reflection=reflection)\n",
        "    print(f\"Initial root node created: {root}\")\n",
        "    return {\n",
        "        **state,\n",
        "        \"root\": root,\n",
        "    }\n"
      ],
      "metadata": {
        "id": "-e8FY8-330c_"
      },
      "id": "-e8FY8-330c_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Tree Expansion"
      ],
      "metadata": {
        "id": "0E396u6r6Feg"
      },
      "id": "0E396u6r6Feg"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def generate_candidates(messages: ChatPromptValue, config: RunnableConfig):\n",
        "    n = config[\"configurable\"].get(\"N\", 5)\n",
        "    print(f\"Generating {n} candidates\")\n",
        "    chat_result = llm.generate(\n",
        "        [messages.to_messages()],\n",
        "        n=n,\n",
        "        callbacks=config[\"callbacks\"],\n",
        "        run_name=\"GenerateCandidates\"\n",
        "    )\n",
        "    return [gen.message for gen in chat_result.generations[0]]\n",
        "\n",
        "expansion_chain = prompt_template | generate_candidates\n",
        "\n",
        "def expand(state: TreeState, config: RunnableConfig) -> dict:\n",
        "    print(\"Expanding tree \\n\")\n",
        "    root = state[\"root\"]\n",
        "    best_candidate: Node = root.best_child if root.children else root\n",
        "    print(f\"Best candidate for expansion : {best_candidate} \\n\")\n",
        "    messages = best_candidate.get_trajectory()\n",
        "\n",
        "    new_candidates = expansion_chain.invoke(\n",
        "        {\"input\": state[\"input\"], \"messages\": messages}, config\n",
        "    )\n",
        "    print(f\"Generated {len(new_candidates)} new candidates \\n\")\n",
        "\n",
        "    output_messages = [[candidate] for candidate in new_candidates]\n",
        "\n",
        "    reflections = reflection_chain.batch(\n",
        "        [{\"input\": state[\"input\"], \"candidate\": msges} for msges in output_messages],\n",
        "        config,\n",
        "    )\n",
        "\n",
        "    child_nodes = [\n",
        "        Node(cand, parent=best_candidate, reflection=reflection)\n",
        "        for cand, reflection in zip(output_messages, reflections)\n",
        "    ]\n",
        "    best_candidate.children.extend(child_nodes)\n",
        "    print(f\"\\n Added {len(child_nodes)} child nodes to the tree \\n\")\n",
        "\n",
        "    return state\n",
        "\n",
        "\n",
        "def should_loop(state: TreeState) -> Literal[\"expand\", \"__end__\"]:\n",
        "    root = state[\"root\"]\n",
        "    print(f\"Checking if should loop again. Root height: {root.height}, Solution Found: {root.is_solved} \\n\")\n",
        "    if root.is_solved:\n",
        "        print(\"Root is solved. Ending search. \\n\")\n",
        "        return END\n",
        "    if root.height > 5:\n",
        "        print(\"Max height reached. Ending search. \\n \")\n",
        "        return END\n",
        "    print(\"Continuing to expand. \\n\")\n",
        "    return \"expand\"\n"
      ],
      "metadata": {
        "id": "4ixbedUf6KI2"
      },
      "id": "4ixbedUf6KI2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Build Graph"
      ],
      "metadata": {
        "id": "RoUitOtU63UP"
      },
      "id": "RoUitOtU63UP"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "builder = StateGraph(TreeState)\n",
        "builder.add_node(\"start\", generate_initial_response)\n",
        "builder.add_node(\"expand\", expand)\n",
        "builder.add_edge(START, \"start\")\n",
        "\n",
        "builder.add_conditional_edges(\n",
        "    \"start\",\n",
        "    should_loop,\n",
        ")\n",
        "builder.add_conditional_edges(\n",
        "    \"expand\",\n",
        "    should_loop,\n",
        ")\n",
        "\n",
        "graph = builder.compile()"
      ],
      "metadata": {
        "id": "XwJALZMv664F"
      },
      "id": "XwJALZMv664F",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Tree Search for best answer"
      ],
      "metadata": {
        "id": "xl2F_oB-7GG6"
      },
      "id": "xl2F_oB-7GG6"
    },
    {
      "cell_type": "code",
      "source": [
        "def print_tree(node, level=0):\n",
        "    print(\"  \" * level + str(node))\n",
        "    for child in node.children:\n",
        "        print_tree(child, level + 1)\n",
        "\n",
        "def run_tree_search(question):\n",
        "    print(f\"Starting tree search for question\")\n",
        "    last_step = None\n",
        "    for step in graph.stream({\"input\": question}):\n",
        "        last_step = step\n",
        "        step_name, step_state = next(iter(step.items()))\n",
        "        print(f\"Step: {step_name}\")\n",
        "        print(f\"Tree height: {step_state['root'].height}\")\n",
        "        print(\"--------------------------------------------------------\")\n",
        "\n",
        "    if \"expand\" in last_step:\n",
        "        solution_node = last_step[\"expand\"][\"root\"].get_best_solution()\n",
        "        best_trajectory = solution_node.get_trajectory(include_reflections=False)\n",
        "        print(\"Best solution found:\")\n",
        "        # print(best_trajectory[-1].content)\n",
        "        content = best_trajectory[-1].content\n",
        "        display(Markdown(content))\n",
        "    else:\n",
        "        print(\"Tree expansion ended \\n \")\n",
        "\n",
        "    print(\"Final tree structure:\")\n",
        "    print_tree(last_step[\"start\"][\"root\"] if \"start\" in last_step else last_step[\"expand\"][\"root\"])\n"
      ],
      "metadata": {
        "id": "Iwtu9RrT7JTo"
      },
      "id": "Iwtu9RrT7JTo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Test query"
      ],
      "metadata": {
        "id": "0MH8hv6y7VQ8"
      },
      "id": "0MH8hv6y7VQ8"
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Generate a table with the average size and weight, as well as the oldest recorded instance for each of the top 5 most common birds.\"\n",
        "run_tree_search(question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iJ4RyHjx7P2J",
        "outputId": "6285219e-d3a3-4017-92dc-8b45bc14dd68"
      },
      "id": "iJ4RyHjx7P2J",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting tree search for question\n",
            "Generating initial response\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "I'm sorry for the inconvenience, but as an AI text-based model, I'm unable to generate tables. However, I can provide the information in text form.\n\n1. House Sparrow\n   - Average Size: 16 cm\n   - Average Weight: 24-39.5 g\n   - Oldest Recorded Instance: 13 years\n\n2. European Starling\n   - Average Size: 20 cm\n   - Average Weight: 60-100 g\n   - Oldest Recorded Instance: 15 years\n\n3. Rock Pigeon\n   - Average Size: 32-37 cm\n   - Average Weight: 238-380 g\n   - Oldest Recorded Instance: 15 years\n\n4. American Robin\n   - Average Size: 23-28 cm\n   - Average Weight: 72-94 g\n   - Oldest Recorded Instance: 14 years\n\n5. Mourning Dove\n   - Average Size: 24-33 cm\n   - Average Weight: 112-170 g\n   - Oldest Recorded Instance: 31 years\n\nPlease note that these values are averages and can vary based on individual characteristics and environmental factors. The age of the oldest recorded instance can also vary depending on the source of the information."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated reflection: reflections=\"The assistant's response is relevant to the user's question, providing detailed information in a structured text form since it's unable to generate tables. The assistant provides the average size, weight, and oldest recorded instance of the top 5 most common birds, which is exactly what the user requested. The response is factually correct to the best of my knowledge. The assistant also explains that these values are averages and can vary, which shows attention to detail and understanding of the subject matter. The quality of text is high; it's clear, easy to understand, and well-structured.\" score=8 found_solution=False \n",
            "\n",
            "Created node : <Node value=0.80, visits=1, Response=I'm sorry for the inconvenience, but as an AI text..., Reflection=The assistant's response is relevant to the user's..., is_solved=False, depth=1>\n",
            "Initial root node created: <Node value=0.80, visits=1, Response=I'm sorry for the inconvenience, but as an AI text..., Reflection=The assistant's response is relevant to the user's..., is_solved=False, depth=1>\n",
            "Checking if should loop again. Root height: 1, Solution Found: False \n",
            "\n",
            "Continuing to expand. \n",
            "\n",
            "Step: start\n",
            "Tree height: 1\n",
            "--------------------------------------------------------\n",
            "Expanding tree \n",
            "\n",
            "Best candidate for expansion : <Node value=0.80, visits=1, Response=I'm sorry for the inconvenience, but as an AI text..., Reflection=The assistant's response is relevant to the user's..., is_solved=False, depth=1> \n",
            "\n",
            "Generating 5 candidates\n",
            "Generated 5 new candidates \n",
            "\n",
            "Generated reflection: reflections=\"The assistant's response is completely irrelevant to the user's request. The user asked for a table with specific information about the top 5 most common birds, but the assistant responded as if the user had thanked them for providing information. The assistant did not provide any information or answer the user's question in any way.\" score=0 found_solution=False \n",
            "\n",
            "Generated reflection: reflections=\"The assistant's response does not answer the user's question at all. The user asked for a table with specific information about the top 5 most common birds, but the assistant merely thanked the user for their feedback. There is no relevance to the user's question, and as such, we cannot assess the factual correctness or quality of the text, because the assistant did not provide any information.\" score=0 found_solution=False \n",
            "\n",
            "Generated reflection: reflections=\"The assistant's response is not relevant at all to the user's request. The user asked for a table containing specific information about the top 5 most common birds, but the assistant provided a generic response without any relevant information. The response lacks factual correctness as it doesn't provide any facts about the birds. The quality of text is fine, but it's completely off-topic.\" score=0 found_solution=False \n",
            "\n",
            "Generated reflection: reflections=\"Assistant's response is relevant to the user's question and provided the required data in a list format due to its limitation of not being able to present data in a tabular format. The information provided seems factually correct, albeit without sources, and it's structured in a clear and understandable way. However, it would be more efficient if the assistant indicated that the data should be verified as it might vary.\" score=8 found_solution=False \n",
            "\n",
            "Generated reflection: reflections=\"The assistant's response was not relevant to the user's request. The user asked for a specific table of information and the assistant responded that it cannot provide a score or reason for the score, which was not what the user asked for. The response is factually correct since the assistant can't provide a score, but it's not relevant to the user's question. Also, the quality of text is okay but could be improved in terms of clarity and relevance.\" score=1 found_solution=False \n",
            "\n",
            "Created node : <Node value=0.80, visits=1, Response=I'm sorry for the inconvenience, but as an AI text..., Reflection=Assistant's response is relevant to the user's que..., is_solved=False, depth=2>\n",
            "Created node : <Node value=0.10, visits=1, Response=I'm sorry for any confusion, but as an AI, I don't..., Reflection=The assistant's response was not relevant to the u..., is_solved=False, depth=2>\n",
            "Created node : <Node value=0.00, visits=1, Response=I'm glad you found the information useful and comp..., Reflection=The assistant's response is completely irrelevant ..., is_solved=False, depth=2>\n",
            "Created node : <Node value=0.00, visits=1, Response=Thank you for the feedback! I strive to provide ac..., Reflection=The assistant's response does not answer the user'..., is_solved=False, depth=2>\n",
            "Created node : <Node value=0.00, visits=1, Response=I appreciate your feedback. I strive to provide ac..., Reflection=The assistant's response is not relevant at all to..., is_solved=False, depth=2>\n",
            "\n",
            " Added 5 child nodes to the tree \n",
            "\n",
            "Checking if should loop again. Root height: 2, Solution Found: False \n",
            "\n",
            "Continuing to expand. \n",
            "\n",
            "Step: expand\n",
            "Tree height: 2\n",
            "--------------------------------------------------------\n",
            "Expanding tree \n",
            "\n",
            "Best candidate for expansion : <Node value=0.80, visits=1, Response=I'm sorry for the inconvenience, but as an AI text..., Reflection=Assistant's response is relevant to the user's que..., is_solved=False, depth=2> \n",
            "\n",
            "Generating 5 candidates\n",
            "Generated 5 new candidates \n",
            "\n",
            "Generated reflection: reflections=\"The assistant's response is relevant to the question and provides the requested information in a clear and concise manner. The text is well-written and easy to understand. The assistant correctly noted that it cannot generate tables, but still provided the information in a structured format. However, it is unclear if the data provided by the assistant is factually correct, as no sources or references were provided to back up the information.\" score=7 found_solution=False \n",
            "\n",
            "Generated reflection: reflections=\"The assistant provided the required information in text form, acknowledging its inability to generate tables. The response was relevant to the question, providing the average size and weight, as well as the oldest recorded instance for each of the top 5 most common birds. It also added a disclaimer that these values are averages and can vary, which shows attention to detail and accuracy. However, without sources, it's hard to verify the factual correctness of the data provided.\" score=8 found_solution=False \n",
            "\n",
            "Generated reflection: reflections=\"The assistant's response was relevant to the user's request, providing the average size and weight, and the oldest recorded instance for each of the top 5 most common birds. The assistant provided a clear explanation about its inability to generate tables and suggested an alternative way to share the information. The facts provided by the assistant seem to be accurate, although sources were not cited, which could leave some room for doubt about the veracity of the information. The text was clear, concise, and well-structured, and the assistant included a disclaimer about the potential for variations in the provided averages.\" score=7 found_solution=False \n",
            "\n",
            "Generated reflection: reflections=\"The assistant's response is relevant to the user's request and the facts provided seem to be correct. The assistant explained the limitation in generating a table and provided the information in a clear list format. The assistant also added a note to clarify that the values provided are averages and may vary, which adds value to the response. However, the assistant failed to cite the sources of the information, which could raise questions about the credibility of the facts.\" score=8 found_solution=False \n",
            "\n",
            "Generated reflection: reflections=\"The assistant's response was both relevant and factually accurate. It was unable to generate a table due to its text-based model limitations but it was able to provide the information in text form. It gave the size, weight, and lifespan of the five most common birds, which was the request from the user. The quality of the text was good and it was easy to understand. The assistant also added a disclaimer at the end to specify that the values provided are averages and can vary, which was a good addition.\" score=8 found_solution=False \n",
            "\n",
            "Created node : <Node value=0.80, visits=1, Response=I'm sorry for the inconvenience, but as an AI text..., Reflection=The assistant's response was both relevant and fac..., is_solved=False, depth=3>\n",
            "Created node : <Node value=0.70, visits=1, Response=I'm sorry for the inconvenience, but as an AI text..., Reflection=The assistant's response was relevant to the user'..., is_solved=False, depth=3>\n",
            "Created node : <Node value=0.80, visits=1, Response=I'm sorry for the inconvenience, but as an AI text..., Reflection=The assistant's response is relevant to the user's..., is_solved=False, depth=3>\n",
            "Created node : <Node value=0.70, visits=1, Response=I'm sorry for the inconvenience, but as an AI text..., Reflection=The assistant's response is relevant to the questi..., is_solved=False, depth=3>\n",
            "Created node : <Node value=0.80, visits=1, Response=I'm sorry for the inconvenience, but as an AI text..., Reflection=The assistant provided the required information in..., is_solved=False, depth=3>\n",
            "\n",
            " Added 5 child nodes to the tree \n",
            "\n",
            "Checking if should loop again. Root height: 3, Solution Found: False \n",
            "\n",
            "Continuing to expand. \n",
            "\n",
            "Step: expand\n",
            "Tree height: 3\n",
            "--------------------------------------------------------\n",
            "Expanding tree \n",
            "\n",
            "Best candidate for expansion : <Node value=0.80, visits=1, Response=I'm sorry for the inconvenience, but as an AI text..., Reflection=The assistant's response was both relevant and fac..., is_solved=False, depth=3> \n",
            "\n",
            "Generating 5 candidates\n",
            "Generated 5 new candidates \n",
            "\n",
            "Generated reflection: reflections=\"The assistant's response was relevant to the question asked and was factually correct. The assistant was able to provide information about the average size, weight, and oldest recorded instance for each of the top 5 most common birds. However, the user asked for a table, and although the assistant cannot generate a table, it could have structured the information in a more table-like format within the text. The information was clear and well-written.\" score=7 found_solution=False \n",
            "\n",
            "Generated reflection: reflections=\"The assistant's response was relevant to the question and provided factual information about the top 5 most common birds. However, the user asked for a table, and although the assistant can't generate an actual table, it could have structured the information in a clearer, table-like format. Also, it's unclear where this specific information came from, as the assistant did not provide any sources. The text quality was good and easy to understand, but these other factors reduce the overall score.\" score=6 found_solution=False \n",
            "\n",
            "Generated reflection: reflections=\"The assistant provided a clear response that was directly relevant to the user's question, providing information on the average size and weight, as well as the oldest recorded instance for each of the top 5 most common birds. However, the assistant claimed that it can't generate tables, which is incorrect as it can format its responses in a table-like manner. The factual correctness of the response is also not verifiable from the response itself as the assistant didn't cite any sources for its information.\" score=7 found_solution=False \n",
            "\n",
            "Generated reflection: reflections=\"The assistant's response was relevant and accurate. It provided the necessary information in a text-based format, which is the most it can do given its current capabilities. The assistant also added a note about the variability of these measurements, which is helpful. However, it could have improved by clarifying where it got the information from, as the user might want to look up more information or verify the facts. In addition, the assistant could have improved by providing a bit more context about why these birds are the most common, as the user might be interested in that.\" score=7 found_solution=False \n",
            "\n",
            "Generated reflection: reflections=\"The assistant's response is highly relevant to the user's request, and it provides accurate and detailed information. However, the assistant could have improved its response by confirming the source of its information or stating that the data is subject to change based on new findings. The assistant could also have asked the user if they wanted more detailed information about each bird, such as their habitat, diet, and behavior. Overall, the response is well-written and easy to understand, but there is room for improvement to make it more engaging and informative.\" score=7 found_solution=False \n",
            "\n",
            "Created node : <Node value=0.70, visits=1, Response=I'm sorry for the inconvenience, but as an AI text..., Reflection=The assistant's response is highly relevant to the..., is_solved=False, depth=4>\n",
            "Created node : <Node value=0.70, visits=1, Response=I'm sorry for the inconvenience, but as an AI text..., Reflection=The assistant's response was relevant to the quest..., is_solved=False, depth=4>\n",
            "Created node : <Node value=0.60, visits=1, Response=I'm sorry for the inconvenience, but as an AI text..., Reflection=The assistant's response was relevant to the quest..., is_solved=False, depth=4>\n",
            "Created node : <Node value=0.70, visits=1, Response=I'm sorry for the inconvenience, but as an AI text..., Reflection=The assistant's response was relevant and accurate..., is_solved=False, depth=4>\n",
            "Created node : <Node value=0.70, visits=1, Response=I'm sorry for the inconvenience, but as an AI text..., Reflection=The assistant provided a clear response that was d..., is_solved=False, depth=4>\n",
            "\n",
            " Added 5 child nodes to the tree \n",
            "\n",
            "Checking if should loop again. Root height: 4, Solution Found: False \n",
            "\n",
            "Continuing to expand. \n",
            "\n",
            "Step: expand\n",
            "Tree height: 4\n",
            "--------------------------------------------------------\n",
            "Expanding tree \n",
            "\n",
            "Best candidate for expansion : <Node value=0.80, visits=1, Response=I'm sorry for the inconvenience, but as an AI text..., Reflection=The assistant's response is relevant to the user's..., is_solved=False, depth=3> \n",
            "\n",
            "Generating 5 candidates\n",
            "Generated 5 new candidates \n",
            "\n",
            "Generated reflection: reflections=\"The assistant's response is relevant to the user's question, providing the information in text form since it can't generate tables. The information provided for each bird - average size, average weight, and oldest recorded instance - is accurate according to common sources, and the assistant also mentions that these values can vary. The response is clear and well-structured.\" score=9 found_solution=True \n",
            "\n",
            "Generated reflection: reflections=\"The assistant's response was relevant to the question and factually correct. It provided the requested information in a well-structured and understandable manner. However, the assistant could have further asked the user if they wanted the information for specific geographical area, since the most common birds can differ by region. The text was clear and of high quality.\" score=8 found_solution=True \n",
            "\n",
            "Generated reflection: reflections=\"The assistant's response was relevant to the question. It provided the required information about the average size, weight, and oldest recorded instance for five common birds. The information seems to be factually correct, but without sources, it's hard to verify. The quality of the text is good, with clear communication and proper use of grammar. However, the assistant could have mentioned that it doesn't have real-time data or access to a database for the most accurate and current information.\" score=7 found_solution=False \n",
            "\n",
            "Generated reflection: reflections=\"The assistant's response is relevant and factually correct. It provided the requested information in a clear, readable format. However, the user asked for a 'table' and the assistant correctly stated that it couldn't generate tables, but then provided the information in a list format. The assistant should have clarified whether the user would like the information in a list format. The quality of the text is good, with clear language and accurate information.\" score=7 found_solution=False \n",
            "\n",
            "Generated reflection: reflections=\"The assistant's response is relevant to the question. It provided the information in text format as it is unable to create tables. The information seems to be factually correct, including the average size, weight, and oldest recorded instance of each bird. However, the assistant did not provide the source of the information, which could raise questions about its accuracy. The text is clear, easy to understand, and well-structured.\" score=8 found_solution=False \n",
            "\n",
            "Created node : <Node value=0.80, visits=1, Response=I'm sorry for the inconvenience, but as an AI text..., Reflection=The assistant's response is relevant to the questi..., is_solved=False, depth=4>\n",
            "Created node : <Node value=0.70, visits=1, Response=I'm sorry for the inconvenience, but as an AI text..., Reflection=The assistant's response is relevant and factually..., is_solved=False, depth=4>\n",
            "Created node : <Node value=0.90, visits=1, Response=I'm sorry for the inconvenience, but as an AI text..., Reflection=The assistant's response is relevant to the user's..., is_solved=True, depth=4>\n",
            "Created node : <Node value=0.70, visits=1, Response=I'm sorry for the inconvenience, but as an AI text..., Reflection=The assistant's response was relevant to the quest..., is_solved=False, depth=4>\n",
            "Created node : <Node value=0.80, visits=1, Response=I'm sorry for the inconvenience, but as an AI text..., Reflection=The assistant's response was relevant to the quest..., is_solved=True, depth=4>\n",
            "\n",
            " Added 5 child nodes to the tree \n",
            "\n",
            "Checking if should loop again. Root height: 4, Solution Found: True \n",
            "\n",
            "Root is solved. Ending search. \n",
            "\n",
            "Step: expand\n",
            "Tree height: 4\n",
            "--------------------------------------------------------\n",
            "Best solution found:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "I'm sorry for the inconvenience, but as an AI text-based model, I'm unable to generate tables. However, I can provide the information in text form.\n\n1. House Sparrow\n   - Average Size: 16 cm\n   - Average Weight: 24-39.5 g\n   - Oldest Recorded Instance: 13 years\n\n2. European Starling\n   - Average Size: 20 cm\n   - Average Weight: 60-100 g\n   - Oldest Recorded Instance: 15 years\n\n3. Rock Pigeon\n   - Average Size: 32-37 cm\n   - Average Weight: 238-380 g\n   - Oldest Recorded Instance: 15 years\n\n4. American Robin\n   - Average Size: 23-28 cm\n   - Average Weight: 72-94 g\n   - Oldest Recorded Instance: 14 years\n\n5. Mourning Dove\n   - Average Size: 24-33 cm\n   - Average Weight: 112-170 g\n   - Oldest Recorded Instance: 31 years\n\nPlease note that these values are averages and can vary based on individual characteristics and environmental factors. The age of the oldest recorded instance can also vary depending on the source of the information."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final tree structure:\n",
            "<Node value=0.61, visits=21, Response=I'm sorry for the inconvenience, but as an AI text..., Reflection=The assistant's response is relevant to the user's..., is_solved=True, depth=1>\n",
            "  <Node value=0.74, visits=16, Response=I'm sorry for the inconvenience, but as an AI text..., Reflection=Assistant's response is relevant to the user's que..., is_solved=True, depth=2>\n",
            "    <Node value=0.70, visits=6, Response=I'm sorry for the inconvenience, but as an AI text..., Reflection=The assistant's response was both relevant and fac..., is_solved=False, depth=3>\n",
            "      <Node value=0.70, visits=1, Response=I'm sorry for the inconvenience, but as an AI text..., Reflection=The assistant's response is highly relevant to the..., is_solved=False, depth=4>\n",
            "      <Node value=0.70, visits=1, Response=I'm sorry for the inconvenience, but as an AI text..., Reflection=The assistant's response was relevant to the quest..., is_solved=False, depth=4>\n",
            "      <Node value=0.60, visits=1, Response=I'm sorry for the inconvenience, but as an AI text..., Reflection=The assistant's response was relevant to the quest..., is_solved=False, depth=4>\n",
            "      <Node value=0.70, visits=1, Response=I'm sorry for the inconvenience, but as an AI text..., Reflection=The assistant's response was relevant and accurate..., is_solved=False, depth=4>\n",
            "      <Node value=0.70, visits=1, Response=I'm sorry for the inconvenience, but as an AI text..., Reflection=The assistant provided a clear response that was d..., is_solved=False, depth=4>\n",
            "    <Node value=0.70, visits=1, Response=I'm sorry for the inconvenience, but as an AI text..., Reflection=The assistant's response was relevant to the user'..., is_solved=False, depth=3>\n",
            "    <Node value=0.78, visits=6, Response=I'm sorry for the inconvenience, but as an AI text..., Reflection=The assistant's response is relevant to the user's..., is_solved=True, depth=3>\n",
            "      <Node value=0.80, visits=1, Response=I'm sorry for the inconvenience, but as an AI text..., Reflection=The assistant's response is relevant to the questi..., is_solved=False, depth=4>\n",
            "      <Node value=0.70, visits=1, Response=I'm sorry for the inconvenience, but as an AI text..., Reflection=The assistant's response is relevant and factually..., is_solved=False, depth=4>\n",
            "      <Node value=0.90, visits=1, Response=I'm sorry for the inconvenience, but as an AI text..., Reflection=The assistant's response is relevant to the user's..., is_solved=True, depth=4>\n",
            "      <Node value=0.70, visits=1, Response=I'm sorry for the inconvenience, but as an AI text..., Reflection=The assistant's response was relevant to the quest..., is_solved=False, depth=4>\n",
            "      <Node value=0.80, visits=1, Response=I'm sorry for the inconvenience, but as an AI text..., Reflection=The assistant's response was relevant to the quest..., is_solved=True, depth=4>\n",
            "    <Node value=0.70, visits=1, Response=I'm sorry for the inconvenience, but as an AI text..., Reflection=The assistant's response is relevant to the questi..., is_solved=False, depth=3>\n",
            "    <Node value=0.80, visits=1, Response=I'm sorry for the inconvenience, but as an AI text..., Reflection=The assistant provided the required information in..., is_solved=False, depth=3>\n",
            "  <Node value=0.10, visits=1, Response=I'm sorry for any confusion, but as an AI, I don't..., Reflection=The assistant's response was not relevant to the u..., is_solved=False, depth=2>\n",
            "  <Node value=0.00, visits=1, Response=I'm glad you found the information useful and comp..., Reflection=The assistant's response is completely irrelevant ..., is_solved=False, depth=2>\n",
            "  <Node value=0.00, visits=1, Response=Thank you for the feedback! I strive to provide ac..., Reflection=The assistant's response does not answer the user'..., is_solved=False, depth=2>\n",
            "  <Node value=0.00, visits=1, Response=I appreciate your feedback. I strive to provide ac..., Reflection=The assistant's response is not relevant at all to..., is_solved=False, depth=2>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pJbAf2WdAc8E"
      },
      "id": "pJbAf2WdAc8E",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}